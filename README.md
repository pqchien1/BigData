# ğŸ˜ Big Data Labs - HÆ°á»›ng dáº«n Chi tiáº¿t tá»« A Ä‘áº¿n Z

> **Dá»± Ã¡n thá»±c hÃ nh Ä‘áº§y Ä‘á»§** vá» xá»­ lÃ½ dá»¯ liá»‡u lá»›n (Big Data) vá»›i Hadoop HDFS, MapReduce, Apache Spark, Spark Streaming vÃ  ElasticSearch - Táº¥t cáº£ cháº¡y trÃªn Docker.

[![Hadoop](https://img.shields.io/badge/Hadoop-3.2.1-yellow?logo=apache-hadoop)](https://hadoop.apache.org/)
[![Spark](https://img.shields.io/badge/Spark-3.1.1-orange?logo=apache-spark)](https://spark.apache.org/)
[![ElasticSearch](https://img.shields.io/badge/ElasticSearch-7.15.2-blue?logo=elasticsearch)](https://www.elastic.co/)
[![Docker](https://img.shields.io/badge/Docker-Required-blue?logo=docker)](https://www.docker.com/)

**ğŸ“Œ HÆ°á»›ng dáº«n nÃ y Ä‘Æ°á»£c viáº¿t CHI TIáº¾T Ä‘á»ƒ báº¡n hiá»ƒu vÃ  lÃ m theo tá»«ng bÆ°á»›c má»™t cÃ¡ch Dá»„ DÃ€NG NHáº¤T!**

---

## ğŸ“– Má»¥c lá»¥c

### Pháº§n 1: Chuáº©n bá»‹
- [ğŸ¯ Tá»•ng quan dá»± Ã¡n](#-tá»•ng-quan-dá»±-Ã¡n)
- [ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng](#ï¸-kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [ğŸ’» YÃªu cáº§u há»‡ thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [âš™ï¸ CÃ i Ä‘áº·t Docker Desktop](#ï¸-cÃ i-Ä‘áº·t-docker-desktop)
- [ğŸš€ Khá»Ÿi Ä‘á»™ng cluster láº§n Ä‘áº§u](#-khá»Ÿi-Ä‘á»™ng-cluster-láº§n-Ä‘áº§u)

### Pháº§n 2: CÃ¡c Labs chi tiáº¿t
- [ğŸ“˜ Lab 1: HDFS - LÆ°u trá»¯ phÃ¢n tÃ¡n](#-lab-1-hdfs---lÆ°u-trá»¯-phÃ¢n-tÃ¡n)
- [ğŸ“— Lab 2: MapReduce - Xá»­ lÃ½ song song](#-lab-2-mapreduce---xá»­-lÃ½-song-song)
- [ğŸ“• Lab 3: ElasticSearch - TÃ¬m kiáº¿m dá»¯ liá»‡u](#-lab-3-elasticsearch---tÃ¬m-kiáº¿m-dá»¯-liá»‡u)
- [ğŸ“™ Lab 4: Spark - Xá»­ lÃ½ nhanh](#-lab-4-spark---xá»­-lÃ½-nhanh)
- [ğŸ“” Lab 5: Spark Streaming - Real-time](#-lab-5-spark-streaming---real-time)

### Pháº§n 3: Váº­n hÃ nh
- [ğŸ–¥ï¸ Web UIs & Monitoring](#ï¸-web-uis--monitoring)
- [âš ï¸ Troubleshooting](#ï¸-troubleshooting)
- [ğŸ’¡ Tips & Best Practices](#-tips--best-practices)

---

## ğŸ¯ Giá»›i thiá»‡u

Dá»± Ã¡n nÃ y cung cáº¥p má»™t **mÃ´i trÆ°á»ng Big Data hoÃ n chá»‰nh** Ä‘Æ°á»£c containerized vá»›i Docker, bao gá»“m:

- **Hadoop Distributed File System (HDFS)** - LÆ°u trá»¯ phÃ¢n tÃ¡n
- **YARN** - Resource management vÃ  job scheduling
- **MapReduce** - Xá»­ lÃ½ dá»¯ liá»‡u batch song song
- **Apache Spark** - Xá»­ lÃ½ dá»¯ liá»‡u nhanh in-memory
- **Spark Streaming** - Xá»­ lÃ½ dá»¯ liá»‡u real-time
- **ElasticSearch** - Full-text search vÃ  analytics
- **Kibana** - Data visualization

### ğŸ“ Má»¥c Ä‘Ã­ch há»c táº­p

1. **Lab 1**: Hiá»ƒu vá» HDFS vÃ  lÆ°u trá»¯ phÃ¢n tÃ¡n
2. **Lab 2**: Láº­p trÃ¬nh MapReduce vá»›i Java
3. **Lab 3**: TÃ¬m kiáº¿m vÃ  phÃ¢n tÃ­ch vá»›i ElasticSearch
4. **Lab 4**: Xá»­ lÃ½ dá»¯ liá»‡u nhanh vá»›i Apache Spark
5. **Lab 5**: Real-time processing vá»›i Spark Streaming

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CLIENT APPLICATIONS                      â”‚
â”‚          (Web UIs, Scripts, Python/Java Programs)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PROCESSING  â”‚              â”‚   SEARCH &      â”‚
â”‚   ENGINES    â”‚              â”‚   ANALYTICS     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MapReduce    â”‚              â”‚ ElasticSearch   â”‚
â”‚ Apache Spark â”‚              â”‚ Kibana          â”‚
â”‚ Spark Stream â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚
        â”‚                              â”‚
        â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           HADOOP DISTRIBUTED FILE SYSTEM             â”‚
â”‚                      (HDFS)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  NameNode  â”‚  DataNode 1  â”‚  DataNode 2             â”‚
â”‚  (Master)  â”‚   (Worker)   â”‚   (Worker)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YARN Resourceâ”‚              â”‚  Spark Cluster  â”‚
â”‚   Manager    â”‚              â”‚                 â”‚
â”‚              â”‚              â”‚ Master + Worker â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

### Core Technologies

| CÃ´ng nghá»‡ | Version | Má»¥c Ä‘Ã­ch | Port |
|-----------|---------|----------|------|
| **Hadoop HDFS** | 3.2.1 | Distributed file storage | 9870 |
| **YARN** | 3.2.1 | Resource management | 8088 |
| **Apache Spark** | 3.1.1 | Fast data processing | 8082 |
| **ElasticSearch** | 7.15.2 | Search & analytics | 9200 |
| **Kibana** | 7.15.2 | Data visualization | 5601 |
| **Docker** | Latest | Containerization | - |

### Programming Languages

- **Java** - MapReduce jobs
- **Python** - Spark jobs (PySpark)
- **Scala** - Spark Streaming jobs

---

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n cá»©ng

| ThÃ nh pháº§n | Tá»‘i thiá»ƒu | Khuyáº¿n nghá»‹ |
|------------|-----------|-------------|
| **RAM** | 8 GB | 16 GB |
| **CPU** | 4 cores | 8 cores |
| **Disk** | 20 GB trá»‘ng | 50 GB |
| **Network** | Internet connection | High-speed |

### Pháº§n má»m

- **OS**: Windows 10/11, macOS, hoáº·c Linux
- **Docker Desktop**: Version 20.10+
- **PowerShell**: Version 5.1+ (Windows)
- **Git**: Optional, Ä‘á»ƒ clone repository

---

## ğŸš€ CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng

### BÆ°á»›c 1: CÃ i Ä‘áº·t Docker Desktop

1. Download Docker Desktop:
   - Windows: https://www.docker.com/products/docker-desktop
   - macOS: https://docs.docker.com/desktop/mac/install/
   - Linux: https://docs.docker.com/engine/install/

2. Cáº¥u hÃ¬nh Docker Desktop:
   ```
   Settings â†’ Resources â†’ Memory: 8GB (khuyáº¿n nghá»‹ 12GB)
   Settings â†’ Resources â†’ CPU: 4 cores (khuyáº¿n nghá»‹ 6 cores)
   Settings â†’ Resources â†’ Disk: 20GB
   ```

3. Khá»Ÿi Ä‘á»™ng Docker Desktop vÃ  Ä‘áº£m báº£o nÃ³ Ä‘ang cháº¡y

### BÆ°á»›c 2: Clone hoáº·c Download Project

```powershell
# Option 1: Clone tá»« Git (náº¿u cÃ³)
git clone <repository-url>
cd "Bai Lab 1.2.3.4.5"

# Option 2: Download vÃ  giáº£i nÃ©n
# Sau Ä‘Ã³ má»Ÿ PowerShell táº¡i thÆ° má»¥c project
```

### BÆ°á»›c 3: Setup mÃ´i trÆ°á»ng

```powershell
# Cháº¡y script setup tá»± Ä‘á»™ng
.\setup.ps1
```

Script `setup.ps1` sáº½ thá»±c hiá»‡n:
- âœ… Kiá»ƒm tra Docker Ä‘ang cháº¡y
- âœ… Pull cÃ¡c Docker images (láº§n Ä‘áº§u ~5-10GB)
- âœ… Khá»Ÿi Ä‘á»™ng toÃ n bá»™ cluster
- âœ… Táº¡o thÆ° má»¥c HDFS cáº§n thiáº¿t
- âœ… Upload dá»¯ liá»‡u máº«u lÃªn HDFS

**â±ï¸ LÆ°u Ã½**: Láº§n Ä‘áº§u tiÃªn sáº½ máº¥t 10-15 phÃºt Ä‘á»ƒ download images.

### BÆ°á»›c 4: Kiá»ƒm tra cÃ i Ä‘áº·t

Má»Ÿ trÃ¬nh duyá»‡t vÃ  kiá»ƒm tra cÃ¡c Web UIs:

```
âœ“ HDFS NameNode:        http://localhost:9870
âœ“ YARN ResourceManager: http://localhost:8088
âœ“ Spark Master:         http://localhost:8082
âœ“ Spark Worker:         http://localhost:8083
```

Kiá»ƒm tra containers Ä‘ang cháº¡y:

```powershell
docker-compose ps
```

Báº¡n sáº½ tháº¥y:
- âœ… namenode
- âœ… datanode1, datanode2
- âœ… resourcemanager
- âœ… nodemanager
- âœ… historyserver
- âœ… spark-master
- âœ… spark-worker-1

---

## ğŸ“š Chi tiáº¿t cÃ¡c Labs

### âš¡ Quick Start cho tá»«ng Lab

- **Lab 1 (HDFS):**
   - Khá»Ÿi Ä‘á»™ng: `docker-compose up -d namenode datanode1 datanode2`
   - Táº¡o thÆ° má»¥c: `docker exec namenode hdfs dfs -mkdir -p /user/hadoop/hdsd`
   - Upload 1GB: `docker exec namenode hdfs dfs -put Lab01/1GB/1GB.bin /user/hadoop/hdsd/data.bin`
   - Kiá»ƒm tra blocks: `docker exec namenode hdfs fsck /user/hadoop/hdsd/data.bin -files -blocks -locations`

- **Lab 2 (MapReduce WordCount):**
   - Upload input: `docker exec namenode hdfs dfs -put Lab02/input_test.txt /user/hadoop/input/`
   - Cháº¡y job: `docker exec namenode hadoop jar /workspace/Lab02/wchdsd.jar WordCount /user/hadoop/input /user/hadoop/wordcount/output`
   - Xem káº¿t quáº£: `docker exec namenode hdfs dfs -cat /user/hadoop/wordcount/output/part-r-00000`

- **Lab 3 (ElasticSearch):**
   - Khá»Ÿi Ä‘á»™ng: `./run-lab3.ps1` hoáº·c `docker-compose --profile lab3 up -d`
   - Kiá»ƒm tra health: `Invoke-RestMethod http://localhost:9200/_cluster/health?pretty`
   - Táº¡o index + doc: dÃ¹ng vÃ­ dá»¥ PowerShell trong pháº§n Lab 3
   - Xem Kibana: http://localhost:5601

- **Lab 4 (Spark - PySpark WordCount):**
   - Cháº¡y: `./run-lab4.ps1` hoáº·c spark-submit nhÆ° vÃ­ dá»¥
   - Input: `hdfs://namenode:9000/user/hadoop/input/input_test.txt`
   - Output: `hdfs://namenode:9000/user/hadoop/spark-output`
   - Xem káº¿t quáº£: `docker exec namenode hdfs dfs -cat /user/hadoop/spark-output/part-*`

- **Lab 5 (Spark Streaming):**
   - Scala: `./run-lab5.ps1` + `docker exec spark-master bash /workspace/Lab05/stream.sh`
   - Python Socket: spark-submit `SocketStreamPython.py` + gá»­i dá»¯ liá»‡u `nc localhost 7777`
   - Python LogAnalyzer: spark-submit `LogAnalyzerPython.py` + stream `nc -l 9999`
   - Theo dÃµi UI: http://localhost:4040

### ğŸ”µ Lab 1: Hadoop HDFS - Distributed File System

**Má»¥c tiÃªu**: Hiá»ƒu cÃ¡ch HDFS lÆ°u trá»¯ vÃ  phÃ¢n tÃ¡n dá»¯ liá»‡u

**Ná»™i dung**:
- Kiáº¿n trÃºc HDFS (NameNode + DataNodes)
- Replication vÃ  fault tolerance
- Block storage (máº·c Ä‘á»‹nh 128MB/block)
- Upload file lá»›n (1GB) vÃ  quan sÃ¡t phÃ¢n tÃ¡n

**CÃ¡c bÆ°á»›c thá»±c hÃ nh**:

1. **Khá»Ÿi Ä‘á»™ng HDFS cluster**:
   ```powershell
   docker-compose up -d namenode datanode1 datanode2
   ```

2. **Táº¡o thÆ° má»¥c trong HDFS**:
   ```powershell
   docker exec namenode hdfs dfs -mkdir -p /user/hadoop
   ```

3. **Upload file 1GB**:
   ```powershell
   docker exec namenode hdfs dfs -put Lab01/1GB/1GB.bin /user/hadoop/hdsd/data.bin
   ```

4. **Kiá»ƒm tra phÃ¢n tÃ¡n blocks**:
   ```powershell
   docker exec namenode hdfs fsck /user/hadoop/hdsd/data.bin -files -blocks -locations
   ```

**Káº¿t quáº£ mong Ä‘á»£i**:
- File 1GB Ä‘Æ°á»£c chia thÃ nh 8 blocks (~128MB/block)
- Má»—i block Ä‘Æ°á»£c replicate 2 láº§n (vÃ¬ cÃ³ 2 datanodes)
 - ThÆ° má»¥c xuáº¥t hiá»‡n trong HDFS: `/user/hadoop/hdsd/`

**XÃ¡c minh nhanh**:
- Má»Ÿ HDFS NameNode UI: http://localhost:9870 â†’ "Utilities â†’ Browse the file system" â†’ kiá»ƒm tra `user/hadoop/hdsd`
- Cháº¡y bÃ¡o cÃ¡o cluster: `docker exec namenode hdfs dfsadmin -report` â†’ tháº¥y 2 live datanodes
- DÃ² block/replication: `docker exec namenode hdfs fsck /user/hadoop/hdsd/data.bin -files -blocks -locations`

**Troubleshooting (Lab 1)**:
- `hdfs dfs` lá»—i: Ä‘áº£m báº£o containers `namenode`, `datanode1`, `datanode2` Ä‘ang cháº¡y (`docker-compose ps`)
- ThÆ° má»¥c khÃ´ng tháº¥y trÃªn UI: refresh UI hoáº·c kiá»ƒm tra Ä‘Æ°á»ng dáº«n Ä‘Ãºng `/user/hadoop/hdsd`
- Thiáº¿u dung lÆ°á»£ng: xÃ³a dá»¯ liá»‡u test cÅ© `docker exec namenode hdfs dfs -rm -r /user/hadoop/old-data`
- Web UI hiá»ƒn thá»‹: http://localhost:9870

---

### ğŸŸ¢ Lab 2: Hadoop MapReduce - Word Count

**Má»¥c tiÃªu**: Láº­p trÃ¬nh MapReduce vá»›i Java Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u lá»›n

**Ná»™i dung**:
- MapReduce paradigm (Map â†’ Shuffle â†’ Reduce)
- Viáº¿t Mapper vÃ  Reducer vá»›i Java
- Compile vÃ  Ä‘Ã³ng gÃ³i JAR
- Cháº¡y job trÃªn YARN cluster

**Source Code**: `Lab02/WordCount/src/WordCount.java`

**Kiáº¿n trÃºc MapReduce**:
```
Input File (HDFS)
     â†“
  Mapper (split & emit)
     â†“
  Shuffle & Sort
     â†“
  Reducer (aggregate)
     â†“
Output File (HDFS)
```

**Cháº¡y Lab 2**:

```powershell
.\run-lab2.ps1
```

Hoáº·c thá»§ cÃ´ng:

```powershell
# 1. Upload input file lÃªn HDFS
docker exec namenode hdfs dfs -put Lab02/input_test.txt /user/hadoop/input/

# 2. Cháº¡y MapReduce job
docker exec namenode hadoop jar /workspace/Lab02/wchdsd.jar WordCount /user/hadoop/input /user/hadoop/wordcount/output

# 3. Xem káº¿t quáº£
docker exec namenode hdfs dfs -cat /user/hadoop/wordcount/output/part-r-00000
```

**Giáº£i thÃ­ch code**:

```java
// Mapper: TÃ¡ch tá»« vÃ  emit (word, 1)
public void map(Object key, Text value, Context context) {
    StringTokenizer itr = new StringTokenizer(value.toString());
    while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);  // emit (word, 1)
    }
}

// Reducer: Tá»•ng há»£p count
public void reduce(Text key, Iterable<IntWritable> values, Context context) {
    int sum = 0;
    for (IntWritable val : values) {
        sum += val.get();
    }
    result.set(sum);
    context.write(key, result);  // emit (word, total_count)
}
```

**Monitoring**:
- YARN UI: http://localhost:8088
- Job history: http://localhost:19888

**Verification**:
- Output tá»“n táº¡i: `docker exec namenode hdfs dfs -ls /user/hadoop/wordcount/output`
- Ná»™i dung Ä‘Ãºng: `docker exec namenode hdfs dfs -cat /user/hadoop/wordcount/output/part-r-00000 | head -n 10`
- YARN hiá»ƒn thá»‹ tráº¡ng thÃ¡i `SUCCEEDED`: má»Ÿ http://localhost:8088 vÃ  kiá»ƒm tra á»©ng dá»¥ng gáº§n nháº¥t.

**Troubleshooting**:
- `Class not found`: Ä‘áº£m báº£o tÃªn class `WordCount` Ä‘Ãºng trong lá»‡nh `hadoop jar`.
- `No such file or directory`: kiá»ƒm tra Ä‘Æ°á»ng dáº«n HDFS Ä‘áº§u vÃ o `/user/hadoop/input` Ä‘Ã£ cÃ³ file.
- `Output exists`: xÃ³a output cÅ© trÆ°á»›c khi cháº¡y láº¡i: `docker exec namenode hdfs dfs -rm -r /user/hadoop/wordcount/output`

---

### ğŸ”´ Lab 3: ElasticSearch & Kibana - Search Engine

**Má»¥c tiÃªu**: XÃ¢y dá»±ng há»‡ thá»‘ng tÃ¬m kiáº¿m vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u

**Ná»™i dung**:
- ElasticSearch cluster (1 master + 2 data nodes)
- Indexing vÃ  searching
- Sharding vÃ  replication
- Kibana visualization

**Khá»Ÿi Ä‘á»™ng Lab 3**:

```powershell
.\run-lab3.ps1
# hoáº·c
docker-compose --profile lab3 up -d
```

**CÃ¡c thÃ nh pháº§n**:
- `elasticsearch-master`: Master node (khÃ´ng lÆ°u data)
- `elasticsearch-data1`: Data node 1
- `elasticsearch-data2`: Data node 2
- `kibana`: Web UI

**Thao tÃ¡c cÆ¡ báº£n**:

```powershell
# 1. Kiá»ƒm tra cluster health
Invoke-RestMethod -Uri "http://localhost:9200/_cluster/health?pretty"

# 2. Táº¡o index vÃ  thÃªm document
$data = @{
    title = "Big Data Lab"
    content = "Learning Hadoop and Spark"
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:9200/test-index/_doc/1" -Method Put -Body $data -ContentType "application/json"

# 3. Search
Invoke-RestMethod -Uri "http://localhost:9200/test-index/_search?q=Hadoop"

# 4. Xem shard distribution
Invoke-RestMethod -Uri "http://localhost:9200/_cat/shards/test-index?v"
```

**Web UIs**:
- ElasticSearch: http://localhost:9200
- Kibana: http://localhost:5601

**Kibana Dev Tools Console**:
```json
GET _cluster/health
GET _cat/nodes?v
GET test-index/_search
```

**Verification**:
- Cluster health `green` hoáº·c `yellow`: `Invoke-RestMethod http://localhost:9200/_cluster/health?pretty`
- Node list Ä‘áº§y Ä‘á»§ (master + 2 data): `Invoke-RestMethod http://localhost:9200/_cat/nodes?v`
- Shards Ä‘Æ°á»£c phÃ¢n phá»‘i: `Invoke-RestMethod http://localhost:9200/_cat/shards?v`
- Táº¡o index vÃ  tÃ¬m kiáº¿m hoáº¡t Ä‘á»™ng: dÃ¹ng vÃ­ dá»¥ táº¡o `test-index` vÃ  `GET test-index/_search` tráº£ vá» hits.

**Troubleshooting**:
- Cháº­m khá»Ÿi Ä‘á»™ng hoáº·c lá»—i bá»™ nhá»›: giáº£m ES heap qua env `ES_JAVA_OPTS=-Xms512m -Xmx512m` vÃ  táº¯t `bootstrap.memory_lock` náº¿u cáº§n.
- `Connection refused`: kiá»ƒm tra container `elasticsearch-*` Ä‘Ã£ cháº¡y (`docker-compose ps`) vÃ  port `9200`/`5601` má»Ÿ.
- `red` health: xÃ³a index lá»—i hoáº·c kiá»ƒm tra logs `docker-compose logs -f elasticsearch-master`.

---

### ğŸŸ¡ Lab 4: Apache Spark - Fast Data Processing

**Má»¥c tiÃªu**: Xá»­ lÃ½ dá»¯ liá»‡u nhanh hÆ¡n MapReduce 10-100 láº§n

**Ná»™i dung**:
- RDD (Resilient Distributed Dataset)
- Transformations vÃ  Actions
- In-memory computing
- PySpark programming

**ChÆ°Æ¡ng trÃ¬nh**:

1. **WordCount.py**: Word count cÆ¡ báº£n
2. **SparkWordCount.py**: Word count nÃ¢ng cao vá»›i threshold

**Cháº¡y Lab 4**:

```powershell
.\run-lab4.ps1
```

Hoáº·c thá»§ cÃ´ng:

```powershell
# Cháº¡y WordCount.py
docker exec spark-master spark-submit \
  --master local[*] \
  /workspace/Lab04/WordCount.py \
  hdfs://namenode:9000/user/hadoop/input/input_test.txt \
  hdfs://namenode:9000/user/hadoop/spark-output

# Xem káº¿t quáº£
docker exec namenode hdfs dfs -cat /user/hadoop/spark-output/part-*
```

**So sÃ¡nh Spark vs MapReduce**:

| Feature | MapReduce | Spark |
|---------|-----------|-------|
| **Speed** | Baseline | 10-100x nhanh hÆ¡n |
| **Storage** | Disk-based | In-memory |
| **API** | Java complex | Python/Scala simple |
| **Use case** | Batch processing | Batch + Streaming + ML |

**Code example (PySpark)**:

```python
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")

# Read tá»« HDFS
text_file = sc.textFile("hdfs://namenode:9000/user/hadoop/input/input.txt")

# Map-Reduce vá»›i Spark
counts = text_file.flatMap(lambda line: line.split(" ")) \
                  .map(lambda word: (word, 1)) \
                  .reduceByKey(lambda a, b: a + b)

# Save káº¿t quáº£
counts.saveAsTextFile("hdfs://namenode:9000/user/hadoop/output")
```

**Monitoring**:
- Spark Master UI: http://localhost:8082
- Spark Application UI: http://localhost:4040 (khi job cháº¡y)

**Verification**:
- Output tá»“n táº¡i: `docker exec namenode hdfs dfs -ls /user/hadoop/spark-output`
- Ná»™i dung Ä‘Ãºng: `docker exec namenode hdfs dfs -cat /user/hadoop/spark-output/part-* | head -n 10`
- á»¨ng dá»¥ng hiá»ƒn thá»‹ trong UI 4040 khi cháº¡y vÃ  hoÃ n táº¥t vá»›i status `Succeeded`.

**Troubleshooting**:
- OOM hoáº·c `Executor Lost`: giáº£m kÃ­ch thÆ°á»›c input, tÄƒng memory: thÃªm `--driver-memory 1g --executor-memory 1g` vÃ o `spark-submit`.
- `File not found`: xÃ¡c nháº­n Ä‘Æ°á»ng dáº«n HDFS input Ä‘Ãºng vÃ  cÃ³ dá»¯ liá»‡u.
- Job treo lÃ¢u: chuyá»ƒn `--master local[*]` khi cháº¡y thá»­, kiá»ƒm tra logs `docker-compose logs -f spark-master`.

---

### ğŸŸ£ Lab 5: Spark Streaming - Real-time Processing

**Má»¥c tiÃªu**: Xá»­ lÃ½ dá»¯ liá»‡u real-time vá»›i window operations

**Ná»™i dung**:
- DStream (Discretized Stream)
- Window operations (30s window, 10s slide)
- Apache log analysis
- Socket streaming

**ChÆ°Æ¡ng trÃ¬nh**:

1. **SocketStream.scala**: Stream cÆ¡ báº£n, lá»c "error"
2. **LogAnalyzerStreaming.scala**: PhÃ¢n tÃ­ch Apache access logs

**Cháº¡y Lab 5** (cáº§n 2 terminals):

**Terminal 1** - Start Spark Streaming:
```powershell
.\run-lab5.ps1
# Chá»n program (1 hoáº·c 2)
```

**Terminal 2** - Stream data vÃ o port:
```powershell
# Stream log file
docker exec spark-master bash /workspace/Lab05/stream.sh
```

Hoáº·c cháº¡y trá»±c tiáº¿p báº£n Python (khuyÃªn dÃ¹ng náº¿u thiáº¿u SBT/Scala):

**Option A - SocketStream (Python):**

1) Terminal A: cháº¡y Spark Streaming
```powershell
docker exec spark-master bash -c "cd /workspace/Lab05 && /spark/bin/spark-submit --master local[*] SocketStreamPython.py"
```

2) Terminal B: gá»­i dá»¯ liá»‡u vÃ o port 7777
```powershell
docker exec -it spark-master bash -c "\
for i in {1..10}; do \
   echo 'INFO: record ' $i; \
   echo 'ERROR: failed at ' $i; \
   sleep 1; \
done | nc localhost 7777\
"
```

Ká»³ vá»ng hiá»ƒn thá»‹ (Terminal A): chá»‰ cÃ¡c dÃ²ng chá»©a "ERROR" theo tá»«ng batch thá»i gian.

**Option B - LogAnalyzer (Python):**

1) Terminal A: cháº¡y Spark Streaming analyzer
```powershell
docker exec spark-master bash -c "cd /workspace/Lab05 && /spark/bin/spark-submit --master local[*] LogAnalyzerPython.py"
```

2) Terminal B: stream log Apache máº«u vÃ o port 9999
```powershell
docker exec -it spark-master bash -c "\
while true; do \
   head -n 100 /workspace/Lab05/log.txt | nc -l 9999; \
   sleep 12; \
done\
"
```

Ká»³ vá»ng hiá»ƒn thá»‹ má»—i 10s: tá»•ng sá»‘ logs, thá»‘ng kÃª content size (avg/min/max), phÃ¢n bá»‘ response code, top IPs (>10 láº§n) vÃ  top endpoints.

**Window Operations**:

```scala
val windowedCounts = lines
  .window(Seconds(30), Seconds(10))  // 30s window, slide 10s
  .flatMap(_.split(" "))
  .map(word => (word, 1))
  .reduceByKey(_ + _)
```

**Apache Log Analysis**:

PhÃ¢n tÃ­ch real-time:
- Response code distribution (200, 404, 500...)
- Content size statistics
- Top 10 endpoints
- Frequent IP addresses

**Dá»«ng job:** Spark Streaming cháº¡y liÃªn tá»¥c Ä‘á»ƒ láº¯ng nghe dá»¯ liá»‡u. Dá»«ng báº±ng phÃ­m Ctrl+C táº¡i terminal Ä‘ang cháº¡y spark-submit, hoáº·c dÃ¹ng `timeout`:

```powershell
docker exec spark-master bash -c "timeout 60s /spark/bin/spark-submit --master local[*] SocketStreamPython.py"
```

**Troubleshooting nhanh (Streaming):**
- Connection refused: cáº§n khá»Ÿi cháº¡y `nc -l <port>` (listener) TRÆ¯á»šC khi spark káº¿t ná»‘i
- KhÃ´ng cÃ³ dá»¯ liá»‡u: kiá»ƒm tra lá»‡nh gá»­i dá»¯ liá»‡u vÃ  port Ä‘Ãºng (7777/9999)
- Cháº­m/máº¥t dá»¯ liá»‡u: tÄƒng batch interval hoáº·c giáº£m tá»‘c Ä‘á»™ gá»­i
- Theo dÃµi UI: má»Ÿ http://localhost:4040 khi job Ä‘ang cháº¡y

**Monitoring**:
- Spark Streaming UI: http://localhost:4040

---

## ğŸ“– HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Quáº£n lÃ½ Cluster

```powershell
# Xem tráº¡ng thÃ¡i containers
docker-compose ps

# Khá»Ÿi Ä‘á»™ng toÃ n bá»™ cluster
docker-compose up -d

# Dá»«ng cluster
docker-compose down

# Dá»«ng vÃ  xÃ³a volumes (XÃ“A DATA)
docker-compose down -v

# Restart má»™t service
docker-compose restart namenode

# Xem logs
docker-compose logs -f namenode
docker-compose logs -f spark-master

# Xem resource usage
docker stats
```

### LÃ m viá»‡c vá»›i HDFS

```powershell
# VÃ o container namenode
docker exec -it namenode bash

# HDFS commands (trong container)
hdfs dfs -ls /                          # List root
hdfs dfs -ls /user/hadoop              # List directory
hdfs dfs -mkdir -p /user/hadoop/test   # Create directory
hdfs dfs -put local.txt /user/hadoop/  # Upload file
hdfs dfs -get /user/hadoop/file.txt ./ # Download file
hdfs dfs -cat /user/hadoop/file.txt    # View file
hdfs dfs -rm /user/hadoop/file.txt     # Delete file
hdfs dfs -rm -r /user/hadoop/dir       # Delete directory

# HDFS admin
hdfs dfsadmin -report                  # Cluster report
hdfs fsck / -files -blocks -locations  # File system check
```

### LÃ m viá»‡c vá»›i Spark

```powershell
# VÃ o Spark master container
docker exec -it spark-master bash

# Spark shell (Scala)
spark-shell --master local[*]

# PySpark shell
pyspark --master local[*]

# Submit Spark job
spark-submit \
  --master local[*] \
  --executor-memory 2g \
  --total-executor-cores 4 \
  your-script.py

# Submit vá»›i HDFS
spark-submit \
  --master local[*] \
  your-script.py \
  hdfs://namenode:9000/input \
  hdfs://namenode:9000/output
```

### LÃ m viá»‡c vá»›i ElasticSearch

```powershell
# REST API examples

# Cluster health
Invoke-RestMethod http://localhost:9200/_cluster/health?pretty

# List nodes
Invoke-RestMethod http://localhost:9200/_cat/nodes?v

# List indices
Invoke-RestMethod http://localhost:9200/_cat/indices?v

# Create index
Invoke-RestMethod -Method Put http://localhost:9200/my-index

# Add document
$doc = @{ title="Test"; content="Hello" } | ConvertTo-Json
Invoke-RestMethod -Method Post -Uri http://localhost:9200/my-index/_doc -Body $doc -ContentType "application/json"

# Search
Invoke-RestMethod http://localhost:9200/my-index/_search?q=Hello
```

---

## ğŸ–¥ï¸ Web UIs & Monitoring

### Hadoop Ecosystem

| Service | URL | MÃ´ táº£ |
|---------|-----|-------|
| **HDFS NameNode** | http://localhost:9870 | Browse HDFS, xem datanodes, blocks |
| **YARN ResourceManager** | http://localhost:8088 | Xem jobs, applications, cluster metrics |
| **Spark Master** | http://localhost:8082 | Xem workers, running applications |
| **Spark Worker** | http://localhost:8083 | Worker details, executor info |
| **Spark Application** | http://localhost:4040 | Job details (chá»‰ khi job cháº¡y) |
| **Job History** | http://localhost:19888 | YARN job history |

### ElasticSearch Stack

| Service | URL | MÃ´ táº£ |
|---------|-----|-------|
| **ElasticSearch** | http://localhost:9200 | REST API endpoint |
| **Kibana** | http://localhost:5601 | Data exploration & visualization |

### Monitoring tá»« Command Line

```powershell
# Container resources
docker stats

# HDFS cluster report
docker exec namenode hdfs dfsadmin -report

# YARN applications
docker exec resourcemanager yarn application -list

# Spark applications
docker exec spark-master curl http://localhost:8080/json/
```

---

## âš ï¸ Troubleshooting

### Problem 1: Docker khÃ´ng khá»Ÿi Ä‘á»™ng

**Triá»‡u chá»©ng**: `docker-compose up` failed

**Giáº£i phÃ¡p**:
```powershell
# Kiá»ƒm tra Docker Desktop Ä‘ang cháº¡y
Get-Process "Docker Desktop"

# Restart Docker Desktop
# Hoáº·c tá»« UI: Right-click Docker icon â†’ Restart
```

### Problem 2: Containers khÃ´ng healthy

**Triá»‡u chá»©ng**: Container status = "unhealthy"

**Giáº£i phÃ¡p**:
```powershell
# Xem logs
docker-compose logs namenode

# Restart container
docker-compose restart namenode

# Náº¿u váº«n lá»—i, restart toÃ n bá»™
docker-compose down
docker-compose up -d
```

### Problem 3: Out of memory

**Triá»‡u chá»©ng**: Container bá»‹ kill, application failed

**Giáº£i phÃ¡p**:

1. TÄƒng memory cho Docker Desktop:
   - Settings â†’ Resources â†’ Memory â†’ 12GB
   - Apply & Restart

2. Giáº£m resource requirements trong `docker-compose.yml`:
   ```yaml
   spark-worker-1:
     environment:
       - SPARK_WORKER_MEMORY=2g  # Giáº£m tá»« 4g
   ```

3. Giáº£m memory cho ElasticSearch:
   ```yaml
   elasticsearch-master:
     environment:
       - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  # Giáº£m tá»« 2g
   ```

### Problem 4: Port Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng

**Triá»‡u chá»©ng**: `port is already allocated`

**Giáº£i phÃ¡p**:

Sá»­a `docker-compose.yml`, Ä‘á»•i port bÃªn trÃ¡i:
```yaml
ports:
  - "19870:9870"  # Thay vÃ¬ 9870:9870
```

Hoáº·c tÃ¬m vÃ  kill process Ä‘ang dÃ¹ng port:
```powershell
# TÃ¬m process
netstat -ano | findstr :9870

# Kill process
taskkill /PID <PID> /F
```

### Problem 5: HDFS khÃ´ng accessible

**Triá»‡u chá»©ng**: `hdfs dfs` commands fail

**Giáº£i phÃ¡p**:
```powershell
# Kiá»ƒm tra namenode
docker exec namenode hdfs dfsadmin -report

# Náº¿u cáº§n, format namenode (XÃ“A DATA!)
docker exec namenode hdfs namenode -format

# Restart HDFS
docker-compose restart namenode datanode1 datanode2
```

### Problem 6: ElasticSearch slow startup

**Triá»‡u chá»©ng**: Cluster khÃ´ng green sau vÃ i phÃºt

**Giáº£i phÃ¡p**:

ÄÃ£ Ä‘Æ°á»£c tá»‘i Æ°u trong config hiá»‡n táº¡i:
- Memory: 512MB/node (thay vÃ¬ 2GB)
- `bootstrap.memory_lock=false`
- Startup time: ~20-30 giÃ¢y

Náº¿u váº«n cháº­m:
```powershell
# Xem logs
docker logs elasticsearch-master

# Restart
docker-compose --profile lab3 restart
```

### Problem 7: Spark job failed

**Triá»‡u chá»©ng**: Job crash hoáº·c stuck

**Giáº£i phÃ¡p**:
```powershell
# Xem logs
docker exec spark-master cat /spark/logs/*

# Kiá»ƒm tra Spark UI
# http://localhost:8082

# Kiá»ƒm tra HDFS connection
docker exec spark-master hdfs dfs -ls /

# Restart Spark
docker-compose restart spark-master spark-worker-1
```

---

## ğŸ’¡ Best Practices

### 1. Resource Management

- ÄÃ³ng cÃ¡c applications khÃ´ng dÃ¹ng Ä‘á»ƒ tiáº¿t kiá»‡m RAM
- Sá»­ dá»¥ng `docker stats` Ä‘á»ƒ monitor resource usage
- Dá»«ng cluster khi khÃ´ng dÃ¹ng: `docker-compose down`

### 2. Data Management

- Backup data quan trá»ng trÆ°á»›c khi `docker-compose down -v`
- Sá»­ dá»¥ng HDFS replication Ä‘á»ƒ Ä‘áº£m báº£o data safety
- Clean up HDFS thÆ°á»ng xuyÃªn: `hdfs dfs -rm -r /user/hadoop/old-data`

### 3. Development Workflow

```powershell
# 1. Start cluster
docker-compose up -d

# 2. Develop & test locally
# Edit code trong Lab02/, Lab04/, Lab05/

# 3. Upload to HDFS (náº¿u cáº§n)
docker exec namenode hdfs dfs -put local-file.txt /user/hadoop/

# 4. Run job
.\run-lab2.ps1  # hoáº·c lab4, lab5

# 5. Check results
docker exec namenode hdfs dfs -cat /user/hadoop/output/part-*

# 6. Stop cluster when done
docker-compose down
```

### 4. Debugging

- LuÃ´n check logs: `docker-compose logs -f <service>`
- Sá»­ dá»¥ng Web UIs Ä‘á»ƒ monitor
- Test vá»›i small dataset trÆ°á»›c
- Verify HDFS data trÆ°á»›c khi cháº¡y job

---

## ğŸ“‚ Cáº¥u trÃºc Project

```
Bai Lab 1.2.3.4.5/
â”‚
â”œâ”€â”€ docker-compose.yml       # Main cluster configuration
â”œâ”€â”€ hadoop.env               # Hadoop environment variables
â”œâ”€â”€ setup.ps1                # Auto setup script
â”œâ”€â”€ run-lab2.ps1            # Run MapReduce job
â”œâ”€â”€ run-lab3.ps1            # Run ElasticSearch
â”œâ”€â”€ run-lab4.ps1            # Run Spark job
â”œâ”€â”€ run-lab5.ps1            # Run Spark Streaming
â”œâ”€â”€ fix-docker.ps1          # Docker troubleshooting
â”‚
â”œâ”€â”€ Lab01/                   # HDFS Lab
â”‚   â”œâ”€â”€ 1GB/
â”‚   â”‚   â””â”€â”€ 1GB.bin         # 1GB sample file
â”‚   â””â”€â”€ Lab1.pdf            # Lab instructions
â”‚
â”œâ”€â”€ Lab02/                   # MapReduce Lab
â”‚   â”œâ”€â”€ WordCount/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â””â”€â”€ WordCount.java
â”‚   â”‚   â”œâ”€â”€ bin/            # Compiled classes
â”‚   â”‚   â””â”€â”€ lib/            # Dependencies
â”‚   â”œâ”€â”€ wchdsd.jar          # Compiled JAR
â”‚   â”œâ”€â”€ input*.txt          # Input files
â”‚   â””â”€â”€ Lab2.pdf
â”‚
â”œâ”€â”€ Lab03/                   # ElasticSearch Lab
â”‚   â””â”€â”€ Lab3.pdf
â”‚
â”œâ”€â”€ Lab04/                   # Spark Lab
â”‚   â”œâ”€â”€ WordCount.py        # Basic word count
â”‚   â”œâ”€â”€ SparkWordCount.py   # Advanced word count
â”‚   â”œâ”€â”€ WordCount_Local.py  # Local mode
â”‚   â”œâ”€â”€ input/              # Test inputs
â”‚   â””â”€â”€ Lab4.pdf
â”‚
â”œâ”€â”€ Lab05/                   # Spark Streaming Lab
â”‚   â”œâ”€â”€ SocketStream.scala
â”‚   â”œâ”€â”€ LogAnalyzerStreaming.scala
â”‚   â”œâ”€â”€ ApacheAccessLog.scala
â”‚   â”œâ”€â”€ build.sbt           # SBT build file
â”‚   â”œâ”€â”€ log.txt             # Sample Apache logs
â”‚   â”œâ”€â”€ stream.sh           # Data streaming script
â”‚   â”œâ”€â”€ Lab5.pdf
â”‚   â””â”€â”€ Run.txt
â”‚
â”œâ”€â”€ data/                    # Sample datasets
â”‚   â”œâ”€â”€ coinmarket_alltime_1.csv
â”‚   â”œâ”€â”€ coinmarket_alltime_2.csv
â”‚   â”œâ”€â”€ coinmarket_alltime_3.csv
â”‚   â”œâ”€â”€ coinmarket_alltime_4.csv
â”‚   â””â”€â”€ data.csv
â”‚
â””â”€â”€ README.md               # This file
```

---

## ğŸ“– TÃ i liá»‡u tham kháº£o

### Official Documentation

- [Apache Hadoop](https://hadoop.apache.org/docs/stable/)
- [Apache Spark](https://spark.apache.org/docs/latest/)
- [Apache Spark Python API (PySpark)](https://spark.apache.org/docs/latest/api/python/)
- [ElasticSearch Guide](https://www.elastic.co/guide/en/elasticsearch/reference/7.15/index.html)
- [Kibana Guide](https://www.elastic.co/guide/en/kibana/7.15/index.html)

### Docker Images

- [Big Data Europe Hadoop](https://github.com/big-data-europe/docker-hadoop)
- [Big Data Europe Spark](https://github.com/big-data-europe/docker-spark)
- [Official ElasticSearch](https://hub.docker.com/_/elasticsearch)

### Tutorials & Books

- **"Hadoop: The Definitive Guide"** - Tom White
- **"Learning Spark"** - Holden Karau et al.
- **"Elasticsearch: The Definitive Guide"** - Clinton Gormley

### Online Resources

- [Hadoop Tutorial](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
- [Spark Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- [PySpark Examples](https://github.com/apache/spark/tree/master/examples/src/main/python)

---

## ğŸ¤ Contributing

Náº¿u báº¡n tÃ¬m tháº¥y bugs hoáº·c muá»‘n cáº£i thiá»‡n project:

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push vÃ  táº¡o Pull Request

---

## ğŸ“ License

This project is for educational purposes only.

---

## ğŸ‘¨â€ğŸ’» Credits & Contact

**Dá»± Ã¡n thá»±c hÃ nh mÃ´n**: Há»‡ Thá»‘ng PhÃ¢n TÃ¡n vÃ  Xá»­ LÃ½ Dá»¯ Liá»‡u Lá»›n

**Technologies**:
- Apache Hadoop & YARN
- Apache Spark
- ElasticSearch & Kibana
- Docker & Docker Compose

**Created**: 2025

---

## ğŸ“ Learning Outcomes

Sau khi hoÃ n thÃ nh cÃ¡c labs, báº¡n sáº½:

âœ… Hiá»ƒu kiáº¿n trÃºc Hadoop HDFS vÃ  distributed storage  
âœ… Láº­p trÃ¬nh MapReduce vá»›i Java  
âœ… Xá»­ lÃ½ dá»¯ liá»‡u nhanh vá»›i Apache Spark (PySpark)  
âœ… Build real-time streaming applications  
âœ… Implement search engine vá»›i ElasticSearch  
âœ… Monitor vÃ  troubleshoot Big Data applications  
âœ… Deploy distributed systems vá»›i Docker  

---

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:

1. Kiá»ƒm tra [Troubleshooting](#ï¸-troubleshooting) section
2. Xem logs: `docker-compose logs -f <service>`
3. Kiá»ƒm tra Web UIs
4. Restart services: `docker-compose restart`

---

**Happy Learning! ğŸ‰**

*Built with â¤ï¸ using Hadoop, Spark, and ElasticSearch*
